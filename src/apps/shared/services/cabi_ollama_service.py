from src.apps.shared.models.urls import ScraperURL
from src.apps.shared.models.species import CabiSpecies
from django.conf import settings
from pymongo import MongoClient
from datetime import datetime
import requests
import json
import re
import logging
from bson.objectid import ObjectId
from django.db import transaction

logger = logging.getLogger(__name__)

class OllamaCabiService:
    def __init__(self):
        self.client = MongoClient(settings.MONGO_URI)
        self.db = self.client[settings.MONGO_DB_NAME]
        self.collection = self.db["cabi_scraper"]
        self.cabi_url = "https://www.cabidigitallibrary.org/product/qc"

    def extract_and_save_species(self, url):
        """Extrae y procesa documentos no procesados de una URL especÃ­fica."""
        documents = list(self.collection.find({"url": url, "processed": {"$ne": True}}))

        if not documents:
            logger.info(f"ðŸš« No hay documentos pendientes de procesar para CABI ({url}).")
            return

        for document in documents:
            try:
                self.process_document(document["_id"])
            except Exception as e:
                logger.error(f"âŒ Error procesando documento CABI: {e}")

    def process_document(self, mongo_id):
        try:
            document = self.collection.find_one({"_id": ObjectId(mongo_id)})

            if not document:
                logger.warning(f"ðŸš« No se encontrÃ³ el documento con ID {mongo_id} en MongoDB.")
                return

            content = document.get("contenido", "")
            source_url = document.get("source_url", "")

            if not content:
                logger.warning(f"ðŸš« Documento {mongo_id} no tiene contenido.")
                return

            existing_data = self.get_existing_species_data(mongo_id)

            structured_data = self.analyze_content_with_ollama(content, source_url, existing_data)

            if not isinstance(structured_data, dict):
                logger.warning(f"âŒ JSON invÃ¡lido para {mongo_id}, no es un diccionario")
                return

            if self.datos_son_validos(structured_data):
                self.save_species_to_postgres(structured_data, source_url, mongo_id)

                self.collection.update_one(
                    {"_id": ObjectId(mongo_id)},
                    {"$set": {"processed": True, "processed_at": datetime.utcnow()}},
                )
                logger.info(f"âœ… Procesado y guardado en PostgreSQL: {mongo_id}")
            else:
                logger.warning(f"âš ï¸ Datos vacÃ­os para {mongo_id}, no se guardan en PostgreSQL.")

        except Exception as e:
            logger.error(f"ðŸš¨ Error procesando documento CABI {mongo_id}: {e}")

    def get_existing_species_data(self, object_id):
        """Obtiene nombre cientÃ­fico, hospedantes y distribuciÃ³n directamente de MongoDB."""
        document = self.collection.find_one({"_id": ObjectId(object_id)})
        if document:
            return {
                "nombre_cientifico": document.get("nombre_cientifico", ""),
                "hospedantes": document.get("hospedantes", ""),
                "distribucion": document.get("distribucion", "")
            }
        return {}

    def analyze_content_with_ollama(self, content, source_url, existing_data):

        nombre_cientifico = existing_data.get("nombre_cientifico", "")
        hospedantes = existing_data.get("hospedantes", "")
        distribucion = existing_data.get("distribucion", "")

        prompt = f"""
        Organiza el siguiente contenido en **formato JSON**, pero 
        **cada campo que contenga mÃºltiples valores debe estar separado por comas dentro de un string, en lugar de usar un array JSON**.
        **Las secciones `prevencion_control` e `impacto` deben mantenerse como objetos anidados con sus claves correspondientes.**
        **Si un campo no tiene informaciÃ³n, usa `""`.**
        
        **Los siguientes campos ya han sido extraÃ­dos y deben mantenerse sin cambios:**
        - `nombre_cientifico`: {nombre_cientifico}
        - `hospedantes`: {hospedantes}
        - `distribucion`: {distribucion}

        **Contenido restante a analizar por IA:**
        {content}
        
        **Estructura esperada en JSON:** 

        {{
          "nombre_cientifico": "{nombre_cientifico}",
          "nombres_comunes": "",
          "sinonimos": "",
          "descripcion_invasividad": "",
          "distribucion": "{distribucion}",
          "impacto": {{"EconÃ³mico": "", "Ambiental": "", "Social": ""}},
          "habitat": "",
          "ciclo_vida": "",
          "reproduccion": "",
          "hospedantes": "{hospedantes}",
          "sintomas": "",
          "organos_afectados": "",
          "condiciones_ambientales": "",
          "prevencion_control": {{"PrevenciÃ³n": "", "Control": ""}},
          "usos": "",
          "url": "{source_url}",
          "hora": "{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
          "fuente": "{self.cabi_url}"
        }}

        **Instrucciones:**
        Devuelve solo el JSON. **No agregues texto antes o despuÃ©s del JSON.**
         **No uses comillas triples , ni bloques de cÃ³digo (`'''`).**
        - **AsegÃºrate de que el JSON devuelto tenga llaves de apertura y cierre correctamente.**

        1. Extrae los nombres comunes de la especie.
        2. Lista los sinÃ³nimos cientÃ­ficos si estÃ¡n disponibles.
        3. Extrae informaciÃ³n sobre impacto econÃ³mico, ambiental y social.
        4. Describe el hÃ¡bitat donde se encuentra.
        5. Explica el ciclo de vida y los mÃ©todos de reproducciÃ³n.
        6. Describe los sÃ­ntomas y los Ã³rganos afectados en los hospedantes.
        7. Extrae las condiciones ambientales clave como temperatura, humedad y precipitaciÃ³n.
        8. Extrae informaciÃ³n sobre mÃ©todos de prevenciÃ³n y control.
        9. Lista los usos conocidos de la especie.
            Devuelve solo el JSON con los datos extraÃ­dos, sin texto adicional.
        10 **Evita respuestas como "AquÃ­ estÃ¡ el JSON" o "Formato JSON esperado". Solo envÃ­a el JSON puro.**
        """

        response = requests.post(
            "http://100.122.137.82:11434/api/chat"
,
            json={"model": "llama3:70b", "messages": [{"role": "user", "content": prompt}]},
            stream=True,
        )

        full_response = ""
        for line in response.iter_lines():
            if line:
                try:
                    json_line = json.loads(line.decode("utf-8"))
                    full_response += json_line.get("message", {}).get("content", "")
                except json.JSONDecodeError:
                    logger.error("âŒ Error al decodificar JSON de Ollama")

        match = re.search(r"\{.*\}", full_response, re.DOTALL)
        if match:
            json_text = match.group(0)
            try:
                return json.loads(json_text)
            except json.JSONDecodeError:
                logger.error("âŒ Error al convertir JSON de Ollama")
                return None
        else:
            logger.warning("âš ï¸ No se encontrÃ³ un JSON vÃ¡lido en la respuesta de Ollama.")
            return None
        
    


    def save_species_to_postgres(self, structured_data, source_url, mongo_id):
        try:
            mapeo_campos = {
                "nombre_cientifico": "scientific_name",
                "nombres_comunes": "common_names",
                "sinonimos": "synonyms",
                "descripcion_invasividad": "invasiveness_description",
                "distribucion": "distribution",
                "impacto": "impact",
                "habitat": "habitat",
                "ciclo_vida": "life_cycle",
                "reproduccion": "reproduction",
                "hospedantes": "hosts",
                "sintomas": "symptoms",
                "organos_afectados": "affected_organs",
                "condiciones_ambientales": "environmental_conditions",
                "prevencion_control": "prevention_control",
                "usos": "uses",
                "url": "source_url",
                "hora": "created_at",
                "fuente": "scraper_source"
            }

            # Renombrar los campos del JSON antes de guardarlos en PostgreSQL
            structured_data = {mapeo_campos.get(k, k): v for k, v in structured_data.items()}

            # Convertir listas en cadenas antes de guardar
            def safe_get_string(value):
                if isinstance(value, list):
                    return ", ".join(str(v).strip() for v in value if v)
                return str(value).strip() if value else ""

            for key in structured_data:
                structured_data[key] = safe_get_string(structured_data[key])

            with transaction.atomic():
                species_obj, created = CabiSpecies.objects.update_or_create(
                    source_url=source_url,
                    defaults=structured_data,
                )

                if created:
                    logger.info(f"âœ… Nueva especie guardada en PostgreSQL: {species_obj.scientific_name}")
                else:
                    logger.info(f"ðŸ”„ Especie actualizada en PostgreSQL: {species_obj.scientific_name}")

        except Exception as e:
            logger.error(f"âŒ Error al guardar en PostgreSQL: {str(e)}")




    def datos_son_validos(self, datos, min_campos=2):
        campos_con_datos = sum(bool(datos.get(campo)) for campo in datos)
        return campos_con_datos >= min_campos
    
def safe_get_string(value):
        if isinstance(value, list):
            return ", ".join(str(v).strip() for v in value if v)  
        return str(value).strip() if value else ""  
